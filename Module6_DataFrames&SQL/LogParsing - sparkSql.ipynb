{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd=os.getcwd() #'/home/raghavendr48edu' This is path for instructor-/mnt/home/edureka_121039\n",
    "#cwd\n",
    "'''for part in cwd.split('/'):\n",
    "    if part.lower().startswith('home'):\n",
    "        user_id=part.title()\n",
    "user_id''' # user_id= Edureka_121039\n",
    "for part in cwd.split('/'):\n",
    "    if part.lower().startswith('ragh'):\n",
    "        user_id=part.title()\n",
    "#user_id #'Raghavendr48Edu'\n",
    "\n",
    "app_name= '{0}:apple_logs'.format(user_id) \n",
    "#app_name #'Raghavendr48Edu: RDD HandsOn'\n",
    "\n",
    "conf=SparkConf().setAppName(app_name)\n",
    "\n",
    "# This applicationId goes i/p to SparkConfiguration\n",
    "#sc.applicationId  #'local-1683463288956'\n",
    "\n",
    "def get_hdfs_filepath(file_name):\n",
    "    my_hdfs='/user/{0}'.format(user_id.lower())\n",
    "    return os.path.join(my_hdfs,file_name)\n",
    "\n",
    "spark=SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|109.169.248.247 - - [12/Dec/2015:18:25:11 +0100] GET /administrator/ HTTP/1.1 200 4263 - Mozilla/5.0 (Windows NT 6.0; rv:34.0) Gecko/20100101 Firefox/34.0 -                                                 |\n",
      "|109.169.248.247 - - [12/Dec/2015:18:25:11 +0100] POST /administrator/index.php HTTP/1.1 200 4494 http://almhuette-raith.at/administrator/ Mozilla/5.0 (Windows NT 6.0; rv:34.0) Gecko/20100101 Firefox/34.0 -|\n",
      "|46.72.177.4 - - [12/Dec/2015:18:31:08 +0100] GET /administrator/ HTTP/1.1 200 4263 - Mozilla/5.0 (Windows NT 6.0; rv:34.0) Gecko/20100101 Firefox/34.0 -                                                     |\n",
      "|46.72.177.4 - - [12/Dec/2015:18:31:08 +0100] POST /administrator/index.php HTTP/1.1 200 4494 http://almhuette-raith.at/administrator/ Mozilla/5.0 (Windows NT 6.0; rv:34.0) Gecko/20100101 Firefox/34.0 -    |\n",
      "|83.167.113.100 - - [12/Dec/2015:18:31:25 +0100] GET /administrator/ HTTP/1.1 200 4263 - Mozilla/5.0 (Windows NT 6.0; rv:34.0) Gecko/20100101 Firefox/34.0 -                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=spark.read.text(get_hdfs_filepath(\"access.clean.log\"))\n",
    "#data=spark.read.text(\"/user/edureka_960126/access.clean.log\")\n",
    "data.show(5,truncate=False)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<re.Match object; span=(0, 15), match='83.167.113.100 '>, '83.167.113.100 ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "url='83.167.113.100 - - [12/Dec/2015:18:31:25 +0100] GET /administrator/ HTTP/1.1 200 4263 - Mozilla/5.0 (Windows NT 6.0; rv:34.0) Gecko/20100101 Firefox/34.0 - '\n",
    "host_pattern = '(^\\S+\\.[\\S+\\.]+[\\S+\\.]+\\S)+\\s'\n",
    "re.search(host_pattern,url),re.search(host_pattern,url).group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------------+------+------------------------+------+------------+\n",
      "|host           |timestamp                 |method|endpoint                |status|content_size|\n",
      "+---------------+--------------------------+------+------------------------+------+------------+\n",
      "|109.169.248.247|12/Dec/2015:18:25:11 +0100|GET   |/administrator/         |200   |4263        |\n",
      "|109.169.248.247|12/Dec/2015:18:25:11 +0100| POST |/administrator/index.php|200   |4494        |\n",
      "|46.72.177.4    |12/Dec/2015:18:31:08 +0100|GET   |/administrator/         |200   |4263        |\n",
      "|46.72.177.4    |12/Dec/2015:18:31:08 +0100| POST |/administrator/index.php|200   |4494        |\n",
      "|83.167.113.100 |12/Dec/2015:18:31:25 +0100|GET   |/administrator/         |200   |4263        |\n",
      "|83.167.113.100 |12/Dec/2015:18:31:25 +0100| POST |/administrator/index.php|200   |4494        |\n",
      "|95.29.198.15   |12/Dec/2015:18:32:10 +0100|GET   |/administrator/         |200   |4263        |\n",
      "|95.29.198.15   |12/Dec/2015:18:32:11 +0100| POST |/administrator/index.php|200   |4494        |\n",
      "|109.184.11.34  |12/Dec/2015:18:32:56 +0100|GET   |/administrator/         |200   |4263        |\n",
      "|109.184.11.34  |12/Dec/2015:18:32:56 +0100| POST |/administrator/index.php|200   |4494        |\n",
      "+---------------+--------------------------+------+------------------------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "(2338006, 6)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "host_pattern = '(^\\S+\\.[\\S+\\.]+[\\S+\\.]+\\S)+\\s'\n",
    "datetime_pattern='\\[(\\d+/[A-Z][a-z]{2}/[0-9]{4}:[0-9]{2}:[0-9]{2}:[0-9]{2}\\s\\+\\d{4})]'\n",
    "method='(GET | POST)\\s*'\n",
    "method_url=']\\s*(GET | POST)\\s*(/\\S*)\\s*HTTP/\\S*\\s([0-9]{3})\\s([0-9]*)'\n",
    "\n",
    "logs_df = data.select(regexp_extract('value', host_pattern, 1).alias('host'),\n",
    "                         regexp_extract('value',datetime_pattern , 1).alias('timestamp'),\n",
    "                         regexp_extract('value', method_url , 1).alias('method'),\n",
    "                         regexp_extract('value', method_url, 2).alias('endpoint'),\n",
    "                         regexp_extract('value', method_url, 3).cast('integer').alias('status'),\n",
    "                         regexp_extract('value', method_url, 4).cast('integer').alias('content_size'))\n",
    "\n",
    "logs_df.show(10, truncate=False)\n",
    "\n",
    "logs_df=logs_df.withColumn(\"method\",trim(logs_df[\"method\"]))\n",
    "\n",
    "print((logs_df.count(), len(logs_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Find out how many 404 HTTP codes are in access logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186787"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_df.filter(\"status=404\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|count_404|\n",
      "+---------+\n",
      "|186787   |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.createOrReplaceTempView(\"logs\")\n",
    "\n",
    "spark.sql('Select count(*) as count_404 From logs where status=404').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Find out which URLs are broken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(endpoint='/templates/_system/css/general.css'),\n",
       " Row(endpoint='/templates/_system/css/general.css'),\n",
       " Row(endpoint='/favicon.ico'),\n",
       " Row(endpoint='/icons/text.gif'),\n",
       " Row(endpoint='/templates/_system/css/general.css'),\n",
       " Row(endpoint='/apache-log/error.log.44.gz'),\n",
       " Row(endpoint='/apache-log/access.log.69.gz'),\n",
       " Row(endpoint='/apache-log/error.log.55.gz'),\n",
       " Row(endpoint='/templates/_system/css/general.css'),\n",
       " Row(endpoint='/favicon.ico')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_df.filter(\"status=404\" or \"status=400\").select('endpoint').take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|endpoint                          |\n",
      "+----------------------------------+\n",
      "|/templates/_system/css/general.css|\n",
      "|/templates/_system/css/general.css|\n",
      "|/favicon.ico                      |\n",
      "|/icons/text.gif                   |\n",
      "|/templates/_system/css/general.css|\n",
      "|/apache-log/error.log.44.gz       |\n",
      "|/apache-log/access.log.69.gz      |\n",
      "|/apache-log/error.log.55.gz       |\n",
      "|/templates/_system/css/general.css|\n",
      "|/favicon.ico                      |\n",
      "|/templates/_system/css/general.css|\n",
      "|/templates/_system/css/general.css|\n",
      "|/templates/_system/css/general.css|\n",
      "|/templates/_system/css/general.css|\n",
      "|/templates/_system/css/general.css|\n",
      "|/favicon.ico                      |\n",
      "|/favicon.ico                      |\n",
      "|/templates/_system/css/general.css|\n",
      "|/templates/_system/css/general.css|\n",
      "|/templates/_system/css/general.css|\n",
      "+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Here are some examples of error codes that a web server may present for a broken link:\n",
    "#404 Page Not Found: the page/resource doesn’t exist on the server\n",
    "#400 Bad Request: the host server cannot understand the URL on your page\n",
    "#Bad host: Invalid host name: the server with that name doesn’t exist or is unreachable\n",
    "#Bad URL: Malformed URL (e.g. a missing bracket, extra slashes, wrong protocol, etc.)\n",
    "#Bad Code: Invalid HTTP response code: the server response violates HTTP spec\n",
    "#Empty: the host server returns “empty” responses with no content and no response code\n",
    "#Timeout: Timeout: HTTP requests constantly timed out during the link check\n",
    "#Reset: the host server drops connections. It is either misconfigured or too busy\n",
    "\n",
    "\n",
    "spark.sql('Select endpoint  From logs where  status=404 or status=400 ').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Verify there are no null columns in the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66366, 324)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(data.filter(data['value'].isNull()).count())\n",
    "\n",
    "\n",
    "#so there were no null rows in our data which we loaded from the access logs text file\n",
    "\n",
    "#Let us check the null values in our regex parsed to check any null columns \n",
    "\n",
    "logs_df.filter(logs_df['content_size'].isNull()).count(),logs_df.filter(logs_df['content_size']==0).count()\n",
    "\n",
    "#so the count below shows that there are the null values spread in our various columns as 66366 rows are having null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<b'sum(CAST((host IS NULL) AS INT)) AS `countofhost`'>, Column<b'sum(CAST((timestamp IS NULL) AS INT)) AS `countoftimestamp`'>, Column<b'sum(CAST((method IS NULL) AS INT)) AS `countofmethod`'>, Column<b'sum(CAST((endpoint IS NULL) AS INT)) AS `countofendpoint`'>, Column<b'sum(CAST((status IS NULL) AS INT)) AS `countofstatus`'>, Column<b'sum(CAST((content_size IS NULL) AS INT)) AS `countofcontent_size`'>]\n",
      "+-----------+----------------+-------------+---------------+-------------+-------------------+\n",
      "|countofhost|countoftimestamp|countofmethod|countofendpoint|countofstatus|countofcontent_size|\n",
      "+-----------+----------------+-------------+---------------+-------------+-------------------+\n",
      "|          0|               0|            0|              0|        60032|              66366|\n",
      "+-----------+----------------+-------------+---------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check column wise null values\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "\n",
    "# Build up a list of column expressions, one per column.\n",
    "exprs = [spark_sum(col(col_name).isNull().cast('integer')).alias('countof'+col_name) for col_name in logs_df.columns]\n",
    "print(exprs)\n",
    "\n",
    "# Run the aggregation. The *exprs converts the list of expressions into\n",
    "# variable function arguments.\n",
    "logs_df.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark.sql(\"SELECT \\\n",
    "  SUM(CASE WHEN column1 IS NULL THEN 1 ELSE 0 END) AS count_column1, \\\n",
    "  SUM(CASE WHEN column2 IS NULL THEN 1 ELSE 0 END) AS count_column2, \\\n",
    "  SUM(CASE WHEN column3 IS NULL THEN 1 ELSE 0 END) AS count_column3 \\\n",
    "FROM logs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Describe in sql abd  DF to RDD and Get columns) or Use df.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark.sql(f\"DESCRIBE {'logs'}\").show()\n",
    "\n",
    "+------------+---------+-------+\n",
    "|    col_name|data_type|comment|\n",
    "+------------+---------+-------+\n",
    "|        host|   string|   null|\n",
    "|   timestamp|   string|   null|\n",
    "|      method|   string|   null|\n",
    "|    endpoint|   string|   null|\n",
    "|      status|      int|   null|\n",
    "|content_size|      int|   null|\n",
    "+------------+---------+-------+\n",
    "\n",
    "spark.sql(f\"DESCRIBE {'logs'}\").select(\"col_name\").rdd.collect()\n",
    "[Row(col_name='host'),\n",
    " Row(col_name='timestamp'),\n",
    " Row(col_name='method'),\n",
    " Row(col_name='endpoint'),\n",
    " Row(col_name='status'),\n",
    " Row(col_name='content_size')]\n",
    " \n",
    " spark.sql(f\"DESCRIBE {'logs'}\").select(\"col_name\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "['host', 'timestamp', 'method', 'endpoint', 'status', 'content_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------+--------------+------------+------------------+\n",
      "|count_host|count_timestamp|count_method|count_endpoint|count_status|count_content_size|\n",
      "+----------+---------------+------------+--------------+------------+------------------+\n",
      "|         0|              0|           0|             0|       60032|             66366|\n",
      "+----------+---------------+------------+--------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Generate the SQL statement dynamically\n",
    "sql = \"SELECT \" + \", \".join([f\"COUNT(IF({col} IS NULL, 1, NULL)) AS count_{col}\" for col in logs_df.columns]) + f\" FROM {'logs'}\"\n",
    "\n",
    "# Execute the SQL statement\n",
    "result = spark.sql(sql)\n",
    "\n",
    "# Display the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. Replace null values with constants such as 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+------+--------------------+------+------------+\n",
      "|           host|           timestamp|method|            endpoint|status|content_size|\n",
      "+---------------+--------------------+------+--------------------+------+------------+\n",
      "|109.169.248.247|12/Dec/2015:18:25...|   GET|     /administrator/|   200|        4263|\n",
      "|109.169.248.247|12/Dec/2015:18:25...|  POST|/administrator/in...|   200|        4494|\n",
      "|    46.72.177.4|12/Dec/2015:18:31...|   GET|     /administrator/|   200|        4263|\n",
      "|    46.72.177.4|12/Dec/2015:18:31...|  POST|/administrator/in...|   200|        4494|\n",
      "| 83.167.113.100|12/Dec/2015:18:31...|   GET|     /administrator/|   200|        4263|\n",
      "+---------------+--------------------+------+--------------------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "#changed only these two column as they only have null values\n",
    "\n",
    "logs_df1=logs_df.withColumn('status' , when(logs_df['status'].isNull(), 0 ).otherwise(logs_df['status']))\n",
    "\n",
    "logs_df1=logs_df1.withColumn('content_size' , when(logs_df['content_size'].isNull(), 0 ).otherwise(logs_df1['content_size']))\n",
    "\n",
    "logs_df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------+--------------+------------+------------------+\n",
      "|count_host|count_timestamp|count_method|count_endpoint|count_status|count_content_size|\n",
      "+----------+---------------+------------+--------------+------------+------------------+\n",
      "|      null|           null|        null|          null|       60032|             66690|\n",
      "+----------+---------------+------------+--------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum as spark_sum\n",
    "logs_df1.agg(*[spark_sum((col(c)==0).cast(\"integer\")).alias(f\"count_{c}\") for c in logs_df1.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+------+--------------------+------+------------+\n",
      "|           host|           timestamp|method|            endpoint|status|content_size|\n",
      "+---------------+--------------------+------+--------------------+------+------------+\n",
      "|109.169.248.247|12/Dec/2015:18:25...|   GET|     /administrator/|   200|        4263|\n",
      "|109.169.248.247|12/Dec/2015:18:25...|  POST|/administrator/in...|   200|        4494|\n",
      "|    46.72.177.4|12/Dec/2015:18:31...|   GET|     /administrator/|   200|        4263|\n",
      "|    46.72.177.4|12/Dec/2015:18:31...|  POST|/administrator/in...|   200|        4494|\n",
      "| 83.167.113.100|12/Dec/2015:18:31...|   GET|     /administrator/|   200|        4263|\n",
      "+---------------+--------------------+------+--------------------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df2=logs_df.na.fill(0)\n",
    "logs_df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------+--------------+------------+------------------+\n",
      "|count_host|count_timestamp|count_method|count_endpoint|count_status|count_content_size|\n",
      "+----------+---------------+------------+--------------+------------+------------------+\n",
      "|      null|           null|        null|          null|       60032|             66690|\n",
      "+----------+---------------+------------+--------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df2.agg(*[spark_sum((col(c)==0).cast(\"integer\")).alias(f\"count_{c}\") for c in logs_df2.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+------+--------------------+------+------------+\n",
      "|           host|           timestamp|method|            endpoint|status|content_size|\n",
      "+---------------+--------------------+------+--------------------+------+------------+\n",
      "|109.169.248.247|12/Dec/2015:18:25...|   GET|     /administrator/|   200|        4263|\n",
      "|109.169.248.247|12/Dec/2015:18:25...|  POST|/administrator/in...|   200|        4494|\n",
      "|    46.72.177.4|12/Dec/2015:18:31...|   GET|     /administrator/|   200|        4263|\n",
      "|    46.72.177.4|12/Dec/2015:18:31...|  POST|/administrator/in...|   200|        4494|\n",
      "| 83.167.113.100|12/Dec/2015:18:31...|   GET|     /administrator/|   200|        4263|\n",
      "| 83.167.113.100|12/Dec/2015:18:31...|  POST|/administrator/in...|   200|        4494|\n",
      "|   95.29.198.15|12/Dec/2015:18:32...|   GET|     /administrator/|   200|        4263|\n",
      "|   95.29.198.15|12/Dec/2015:18:32...|  POST|/administrator/in...|   200|        4494|\n",
      "|  109.184.11.34|12/Dec/2015:18:32...|   GET|     /administrator/|   200|        4263|\n",
      "|  109.184.11.34|12/Dec/2015:18:32...|  POST|/administrator/in...|   200|        4494|\n",
      "|   91.227.29.79|12/Dec/2015:18:33...|   GET|     /administrator/|   200|        4263|\n",
      "|   91.227.29.79|12/Dec/2015:18:33...|  POST|/administrator/in...|   200|        4494|\n",
      "|  90.154.66.233|12/Dec/2015:18:36...|   GET|     /administrator/|   200|        4263|\n",
      "|  90.154.66.233|12/Dec/2015:18:36...|  POST|/administrator/in...|   200|        4494|\n",
      "|  95.140.24.131|12/Dec/2015:18:38...|   GET|     /administrator/|   200|        4263|\n",
      "|  95.140.24.131|12/Dec/2015:18:38...|  POST|/administrator/in...|   200|        4494|\n",
      "|  95.188.245.16|12/Dec/2015:18:38...|   GET|     /administrator/|   200|        4263|\n",
      "|  95.188.245.16|12/Dec/2015:18:38...|  POST|/administrator/in...|   200|        4494|\n",
      "|  46.72.213.133|12/Dec/2015:18:39...|   GET|     /administrator/|   200|        4263|\n",
      "|  46.72.213.133|12/Dec/2015:18:39...|  POST|/administrator/in...|   200|        4494|\n",
      "+---------------+--------------------+------+--------------------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the SQL statement dynamically\n",
    "select_expr = \", \".join([f\"COALESCE({col}, 0) AS {col}\" for col in logs_df1.columns])\n",
    "sql = f\"SELECT {select_expr} FROM {'logs'}\"\n",
    "\n",
    "# Execute the SQL statement\n",
    "result = spark.sql(sql)\n",
    "\n",
    "# Show the result\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------+--------------+------------+------------------+\n",
      "|count_host|count_timestamp|count_method|count_endpoint|count_status|count_content_size|\n",
      "+----------+---------------+------------+--------------+------------+------------------+\n",
      "|         0|              0|           0|             0|           0|               324|\n",
      "+----------+---------------+------------+--------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sql=\"SELECT \" + \", \".join([\"SUM(CASE WHEN {} = 0 THEN 1 ELSE 0 END) AS count_{}\".format(col, col) for col in logs_df2.columns]) + \" FROM {}\".format('logs')\n",
    "\n",
    "\n",
    "spark.sql(sql).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Parse timestamp to readable date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+------+--------------------+------+------------+----------+\n",
      "|           host|           timestamp|method|            endpoint|status|content_size|      date|\n",
      "+---------------+--------------------+------+--------------------+------+------------+----------+\n",
      "|109.169.248.247|12/Dec/2015:18:25...|   GET|     /administrator/|   200|        4263|2015-12-12|\n",
      "|109.169.248.247|12/Dec/2015:18:25...|  POST|/administrator/in...|   200|        4494|2015-12-12|\n",
      "|    46.72.177.4|12/Dec/2015:18:31...|   GET|     /administrator/|   200|        4263|2015-12-12|\n",
      "|    46.72.177.4|12/Dec/2015:18:31...|  POST|/administrator/in...|   200|        4494|2015-12-12|\n",
      "| 83.167.113.100|12/Dec/2015:18:31...|   GET|     /administrator/|   200|        4263|2015-12-12|\n",
      "+---------------+--------------------+------+--------------------+------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df1 = logs_df1.withColumn('date', date_format(to_timestamp('timestamp', 'dd/MMM/yyyy:HH:mm:ss'), 'yyyy-MM-dd'))\n",
    "logs_df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+------+--------------------+------+------------+----------+\n",
      "|           host|           timestamp|method|            endpoint|status|content_size|      date|\n",
      "+---------------+--------------------+------+--------------------+------+------------+----------+\n",
      "|109.169.248.247|12/Dec/2015:18:25...|   GET|     /administrator/|   200|        4263|2015-12-12|\n",
      "|109.169.248.247|12/Dec/2015:18:25...|  POST|/administrator/in...|   200|        4494|2015-12-12|\n",
      "|    46.72.177.4|12/Dec/2015:18:31...|   GET|     /administrator/|   200|        4263|2015-12-12|\n",
      "|    46.72.177.4|12/Dec/2015:18:31...|  POST|/administrator/in...|   200|        4494|2015-12-12|\n",
      "| 83.167.113.100|12/Dec/2015:18:31...|   GET|     /administrator/|   200|        4263|2015-12-12|\n",
      "+---------------+--------------------+------+--------------------+------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT *, date_format(to_timestamp(timestamp, 'dd/MMM/yyyy:HH:mm:ss'), 'yyyy-MM-dd') AS date FROM logs \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Describe which HTTP status values appear in data and how many. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|status|count(status)|\n",
      "+------+-------------+\n",
      "|   412|           19|\n",
      "|   406|           53|\n",
      "|  null|            0|\n",
      "|   206|       939908|\n",
      "|   500|         3225|\n",
      "|   301|          609|\n",
      "|   400|           20|\n",
      "|   403|         2217|\n",
      "|   404|       186787|\n",
      "|   200|      1138403|\n",
      "|   303|          244|\n",
      "|   304|         6329|\n",
      "|   405|            7|\n",
      "|   401|          153|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select status,count(status) from logs group by status \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------------------------------------------------------------+\n",
      "|status|count(status) OVER (PARTITION BY status ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)|\n",
      "+------+-------------------------------------------------------------------------------------------------+\n",
      "|   406|                                                                                               53|\n",
      "|   412|                                                                                               19|\n",
      "|  null|                                                                                                0|\n",
      "|   206|                                                                                           939908|\n",
      "|   500|                                                                                             3225|\n",
      "|   301|                                                                                              609|\n",
      "|   400|                                                                                               20|\n",
      "|   403|                                                                                             2217|\n",
      "|   404|                                                                                           186787|\n",
      "|   200|                                                                                          1138403|\n",
      "|   303|                                                                                              244|\n",
      "|   304|                                                                                             6329|\n",
      "|   405|                                                                                                7|\n",
      "|   401|                                                                                              153|\n",
      "+------+-------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select status,count(status) over(PARTITION BY status) from logs \").distinct().show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark.sql(\"select status,count(status) over(PARTITION BY status) as count_status from logs group by status,count_status\").show()\n",
    "\n",
    "Py4JJavaError: An error occurred while calling o26.sql.\n",
    ": org.apache.spark.sql.AnalysisException: aggregate functions are not allowed in GROUP BY, but found count(logs.`status`) OVER (PARTITION BY logs.`status` ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING);;\n",
    "Project [status#11, count_status#1777L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|status|  count|\n",
      "+------+-------+\n",
      "|   406|     53|\n",
      "|   412|     19|\n",
      "|   206| 939908|\n",
      "|   500|   3225|\n",
      "|   301|    609|\n",
      "|   400|     20|\n",
      "|   403|   2217|\n",
      "|   404| 186787|\n",
      "|   200|1138403|\n",
      "|   303|    244|\n",
      "|   304|   6329|\n",
      "|   405|      7|\n",
      "|     0|  60032|\n",
      "|   401|    153|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.partitionBy('status')\n",
    "\n",
    "# Use window functions to count occurrences of each status\n",
    "logs_df1.select(col('status'), count(col('status')).over(window_spec).alias('count')).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|status|  count|\n",
      "+------+-------+\n",
      "|   412|     19|\n",
      "|   406|     53|\n",
      "|   206| 939908|\n",
      "|   500|   3225|\n",
      "|   301|    609|\n",
      "|   400|     20|\n",
      "|   403|   2217|\n",
      "|   404| 186787|\n",
      "|   200|1138403|\n",
      "|   303|    244|\n",
      "|   304|   6329|\n",
      "|   405|      7|\n",
      "|     0|  60032|\n",
      "|   401|    153|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df1.groupBy('status').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. How many unique hosts are there in the entire log and their average request "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark.sql(\"select avg(count(host)) from logs group by host\").show(5)\n",
    "Py4JJavaError: An error occurred while calling o26.sql.\n",
    ": org.apache.spark.sql.AnalysisException: It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.;;\n",
    "Aggregate [host#7], [avg(count(host#7)) AS avg(count(host))#1813]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|         host|count(host)|\n",
      "+-------------+-----------+\n",
      "|  46.72.177.4|          8|\n",
      "|194.48.218.78|          2|\n",
      "|31.181.253.16|          2|\n",
      "| 37.112.46.76|          2|\n",
      "|95.107.90.225|          2|\n",
      "+-------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select host,count(host) from logs group by host\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+\n",
      "|(CAST(count(host) AS DOUBLE) / CAST(count(DISTINCT host) AS DOUBLE))|\n",
      "+--------------------------------------------------------------------+\n",
      "|                                                   57.25355078851993|\n",
      "+--------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(host)/count(distinct host) from logs \").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|(count(host) / count(DISTINCT host))|\n",
      "+------------------------------------+\n",
      "|                   57.25355078851993|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "logs_df1.select(count('host')/countDistinct('host')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
